


import numpy as np
from scipy.stats import bernoulli
from math import log

import random
import sys
import matplotlib.pyplot as plt
%matplotlib inline

import seaborn as sns
colors = sns.color_palette('colorblind')


def plot_regret(regrets, logscale=False, lb=None,q=10):
    """
    regrets must be a dict {'agent_id':regret_table}
    """

    reg_plot = plt.figure()
    #compute useful stats
#     regret_stats = {}
    for i, agent_id in enumerate(regrets.keys()):
        data = regrets[agent_id]
        N, T = data.shape
        cumdata = np.cumsum(data, axis=1) # cumulative regret

        mean_reg = np.mean(cumdata, axis=0)
        q_reg = np.percentile(cumdata, q, axis=0)
        Q_reg = np.percentile(cumdata, 100-q, axis=0)

#         regret_stats[agent_id] = np.array(mean_reg, q_reg, Q_reg)

        plt.plot(np.arange(T), mean_reg, color=colors[i], label=agent_id)
        plt.fill_between(np.arange(T), q_reg, Q_reg, color=colors[i], alpha=0.2)

    if logscale:
        plt.xscale('log')
        plt.xlim(left=100)

    if lb is not None:
        plt.plot(np.arange(T), lb, color='black', marker='*', markevery=int(T/10))

    plt.xlabel('time steps')
    plt.ylabel('Cumulative Regret')
    plt.legend()
    reg_plot.show()








def ActionsGenerator(K,d, mean=None):
    """
    K: int -- number of action vectors to be generated
    d : int -- dimension of the action space
    returns : an array of K vectors uniformly sampled on the unit sphere in R^d
    """
    # Random vector with mean = 0
    rand_vector = np.random.normal(size=(K, d))
    
    # Normalize each vector to have unit norm
    norms = np.linalg.norm(rand_vector, axis=1, keepdims=True)
    unit_vectors = rand_vector / norms

    # If a mean vector is provided, add it to the unit_vector
    if mean is not None:
        unit_vectors += mean
        
        # Normalize again to ensure they remain on the unit sphere
        norms = np.linalg.norm(unit_vectors, axis=1, keepdims=True)
        unit_vectors = unit_vectors / norms

    return unit_vectors





class LinearBandit:

    def __init__(self, theta, K, var=1., fixed_actions=None):
      """
      theta: d-dimensional vector (bounded) representing the hidden parameter
      K: number of actions per round (random action vectors generated each time)
      pb_type: string in 'fixed', 'iid', 'nsr' (please ignore NotSoRandom)
      """
      self.d = np.size(theta)
      self.theta = theta
      self.K = K
      self.var = var
      self.current_action_set = np.zeros(self.d)
      self.fixed_actions = fixed_actions

    def get_action_set(self):
      """
      Generates a set of vectors in dimension self.d. Use your ActionsGenerator
      Alternatively, the set of actions is fixed a priori (given as input).
      Implement a condition to return the fixed set when one is given
      """
      if self.fixed_actions is not None:
        return self.fixed_actions
      else: 
        self.current_action_set = ActionsGenerator(self.K, self.d)
        return ActionsGenerator(self.K, self.d)


    def get_reward(self, action):
      """ sample reward given action and the model of this bandit environment
      action: d-dimensional vector (action chosen by the learner)
      """
      # self.current_action_set = action
      mean = np.dot(action, self.theta)
      return np.random.normal(mean, scale=self.var)

    def get_means(self):
      # print(f'current action_set shape = {self.current_action_set.shape}')
      # print(f'theta shape = {self.theta.shape}')
      return np.dot(self.current_action_set, self.theta)
    
    def __repr__(self):
      str = 'Object Environnement'
      str += f'\n d = {self.d}'
      str += f'\n Theta = {self.theta}, shape = {self.theta.shape}'
      str += f'\n Current_action_set = {self.current_action_set}, shape = {self.current_action_set.shape}'
      
      return str





def play(environment, agent, Nmc, T, pseudo_regret=True):
    """
    Play one Nmc trajectories over a horizon T for the specified agent.
    Return the agent's name (sring) and the collected data in an nd-array.
    """

    data = np.zeros((Nmc, T))

    for n in range(Nmc):
        agent.reset()
        for t in range(T):
            action_set = environment.get_action_set()
            # print(f'action set shape = {action_set.shape}')
            # print(f'env = {env}')
            action = agent.get_action(action_set)
            # print(f'action from get action = {action.shape}')
            reward = environment.get_reward(action)
            agent.receive_reward(action,reward)

            # compute instant (pseudo) regret
            means = environment.get_means()
            best_reward = np.max(means)
            if pseudo_regret:
              # pseudo-regret removes some of the noise and corresponds to the metric studied in class
              data[n,t] = best_reward - np.dot(environment.theta,action)
            else:
              data[n,t]= best_reward - reward # this can be negative due to the noise, but on average it's positive

    return agent.name(), data


def experiment(environment, agents, Nmc, T,pseudo_regret=True):
    """
    Play Nmc trajectories for all agents over a horizon T. Store all the data in a dictionary.
    """

    all_data = {}

    for agent in agents:
        agent_id, regrets = play(environment, agent,Nmc, T,pseudo_regret)

        all_data[agent_id] = regrets

    return all_data











class LinUniform():
  def __init__(self):
    # self.d = d
    # self.lambda_reg = lambda_reg
    # self.reset()
    pass
  
  def get_action(self, arms):
    K, _ = arms.shape
    return arms[np.random.choice(K)]

  def receive_reward(self, chosen_arm, reward):
    # chosen_arm = chosen_arm.reshape(-1, 1)

    # self.action += chosen_arm.T@chosen_arm
    # self.invcov = np.linalg.pinv(self.cov + self.action) 


    # #update b_t
    # self.b_t += reward * chosen_arm

    # self.hat_theta = self.invcov@ self.b_t # update the least square estimate
    pass
  
  def reset(self):
    # self.hat_theta = np.zeros(self.d).reshape(-1, 1)
    # self.cov = self.lambda_reg * np.identity(self.d)
    # self.invcov = np.identity(self.d)
    # self.b_t = np.zeros(self.d).reshape(-1, 1)
    # self.action = np.zeros(self.d).reshape(-1, 1)
    pass
  def name(self):
    return 'Unif'






from numpy.linalg import pinv
from time import time

class LinEpsilonGreedy:
  def __init__(self, d,lambda_reg, eps=0.1, other_option=None):
    self.eps = eps # exploration probability
    self.d = d
    self.lambda_reg = lambda_reg
    self.reset()
    self.other_option = other_option
    #use other inputs if needed

  def reset(self):
    """
    This function should reset all estimators and counts.
    It is used between independent experiments (see 'Play!' above)
    """
    self.t = 0
    self.hat_theta = np.zeros(self.d).reshape(-1, 1)
    self.cov = self.lambda_reg * np.identity(self.d)
    self.invcov = np.identity(self.d)
    self.b_t = np.zeros(self.d).reshape(-1, 1)
    self.action = np.zeros(self.d).reshape(-1, 1)
    

  def greedy_or_not(self, K, arms):
    """This function picks the greedy arm with a probability (p = 1 - epsilon)
    It tries a random arm with a probability (p = epsilon)"""

    threshold = np.random.rand() # Pick random numbers between 0 and 1

    if self.eps < threshold: # Greedy strategy, pick the arm that provides the best rewards
      expected_reward = arms@self.hat_theta
      index = np.argmax(expected_reward) # changer Ã§a
      return arms[index, :]
    else : # Explore strategy, pick a random arm
      K, _ = arms.shape
      return arms[np.random.choice(K)]


  def get_action(self, arms):
    K = arms.shape[0] # number of arms

    action = self.greedy_or_not(K, arms)
    return action
  
  def sherman_morrison(self, P_inv, a):
      denominator = (1 + a.T@P_inv@a)
      numerator = (P_inv@a)@(a.T@P_inv)
      return numerator/denominator
  
  def receive_reward(self, chosen_arm, reward):
    """
    update the internal quantities required to estimate the parameter theta using least squares
    """
    #update inverse covariance matrix
    chosen_arm = chosen_arm.reshape(-1, 1)
    if self.other_option is not None :
      if self.t == 0:
        self.invcov = np.linalg.pinv(self.cov) 
        # self.invcov -= np.copy(self.sherman_morrison(self.invcov, chosen_arm))
      else :
        self.invcov -= self.sherman_morrison(self.invcov, chosen_arm)
    else :
      self.action += chosen_arm.T@chosen_arm
      self.invcov = np.linalg.pinv(self.cov + self.action) 

    #update b_t
    self.b_t += reward * chosen_arm

    self.hat_theta = self.invcov@self.b_t # update the least square estimate
    self.t += 1

  def name(self):
    return 'LinEGreedy('+str(self.eps)+')'
  
  def __repr__(self):
    str = f'Object LinEpsilonGreedy'
    str += f'\nTime step = {self.t}'
    str += f'\nTheta Hat = {self.hat_theta}, shape = {self.hat_theta.shape}'
    str += f'\nCov matrix = {self.cov}, shape = {self.cov.shape}'
    
    return str






d = 15  # dimension
K = 30  # number of arms

# parametor vector \theta, normalized :
theta = ActionsGenerator(1,d).reshape(-1) # another way of getting a 'random vector' (depends on your implementation)
# theta = np.array([0.45, 0.5, 0.5])
theta /= np.linalg.norm(theta)
T = 1000  # Finite Horizon
N = 30  # Monte Carlo simulations

delta = 0.1 # could be set directly in the algorithms
sigma = 1.

#choice of percentile display
q = 10


env = LinearBandit(theta, K, var=sigma**2)

print(env)


# policies

uniform = LinUniform()
e_greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0.1, other_option=None)
greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0., other_option=None)


exp = experiment(env, [greedy, e_greedy, uniform], Nmc=N, T=T,pseudo_regret=True)
plot_regret(exp)











import time

d_vector = np.array([3, 10, 50, 100])
time_vector = np.zeros((2, len(d_vector)))

# Computational time comparison of both matrix inversion techniques
for i, d_val in enumerate(d_vector):
    theta = ActionsGenerator(1,d_val).reshape(-1)
    env_time = LinearBandit(theta, K, var=sigma**2)
    
    start = time.time()
    greedy_1 = LinEpsilonGreedy(d_val, lambda_reg=1., eps=0.1, other_option = True)
    exp1 = experiment(env_time, [greedy_1], Nmc=N, T=T,pseudo_regret=True)
    end = time.time() - start
    plot_regret(exp1)
    time_vector[0, i] = end

    start_2 = time.time()
    greedy_2 = LinEpsilonGreedy(d_val, lambda_reg=1., eps=0.1)
    exp2 = experiment(env_time, [greedy_2], Nmc=N, T=T,pseudo_regret=True)
    end_2 = time.time() - start_2
    time_vector[1, i] = end_2
    plot_regret(exp2)


plt.figure()
plt.semilogy(d_vector, time_vector[0,:], label = "Sherman Morrison formula")
plt.semilogy(d_vector, time_vector[1,:], label = "Classical matrix inversion")
plt.xlabel("d dimension")
plt.ylabel("Computational time - s")
plt.legend()
plt.title("Epsilon greedy computational time comparison")





class LinUCB:
    
  # constructeur - init paramÃ¨tres :
  def __init__(self, d, delta, sigma=sigma, lambda_reg=1., other_option=None, T=1000):
      
    self.d = d
    self.delta = delta
    self.sigma = sigma
    self.lambda_reg = lambda_reg
    self.T = T
    
    self.other_option = other_option   
    self.reset()

  # reset method - used in the play function loop
  def reset(self):
    """
    This function should reset all estimators and counts.
    It is used between independent experiments (see 'Play!' above)
    """
    self.t = 0
    self.theta_hat = np.zeros(self.d).reshape(-1, 1)
    self.cov = self.lambda_reg * np.identity(self.d)
    self.invcov = np.identity(self.d)
    self.history_design_matrix = np.zeros((d,d)) # sum over s of x_s . x_s^{T}
    self.history_rx = np.zeros(d) # sum over s of r_s . x_s

  def _sherman_morrison(self, P_inv, a):
      denominator = (1 + a.T@P_inv@a)
      numerator = (P_inv@a)@(a.T@P_inv)
      return numerator/denominator
    
  def _compute_ucbs(self, K, arms):
          
      # calculate UCB bounds
      ucbs = np.zeros(K)
      for k in range(K):
          action = arms[k]
          beta = self.sigma * np.sqrt(2*np.log(1/self.delta) + self.d * np.log(1+self.t / (self.d * self.lambda_reg))) \
                                      + np.sqrt(self.lambda_reg) * np.linalg.norm(self.theta_hat) 
                                      # hum how do we estimate theta star ? using theta_hat
          norm_action = np.sqrt(np.matmul(action.T, np.matmul(self.invcov, action)))
          # print(f'action shape = {action.shape}')
          # print(f'self.theta_hat reshape = {self.theta_hat.reshape(-1,).shape}')
          ucbs[k] = np.dot(action, self.theta_hat.reshape(-1,)) + norm_action * beta
          # print(f'ucb {k} = {ucbs[k]:.6f}')

    # find argmax and return chosen action
      chosen_arm_id = np.argmax(ucbs)
      chosen_action = arms[chosen_arm_id].reshape(-1,)
      # print(f'best arm = {chosen_arm_id}')
      
      return chosen_arm_id, chosen_action

  # choose the arm per policy
  def get_action(self, arms):
      
    K = arms.shape[0] # number of arms
    chosen_arm_id, chosen_action = self._compute_ucbs(K, arms)
      
    return chosen_action
  
  # update internal parameters
  def receive_reward(self, chosen_arm, reward):
    """
    update the internal quantities required to estimate the parameter theta using least squares
    """
    
    # update sum of xx.T
    self.history_design_matrix += np.matmul(chosen_arm.T, chosen_arm)
    # update sum of rx
    # print(f'reward = {reward}')
    # print(f'history_rx = {self.history_rx}.shape')
    self.history_rx += reward * chosen_arm
    # update cov matrix
    self.cov = self.lambda_reg * np.identity(self.d) + self.history_design_matrix # B_t_lambda
    
    #update inverse covariance matrix, possibly with sherman-morrisson
    # chosen_arm = chosen_arm.reshape(-1, 1)
    
    # if self.other_option is not None :
    #   if self.t == 0:
    #     self.invcov = np.linalg.pinv(self.cov) 
    #   else :
    #     self.invcov -= self._sherman_morrison(self.invcov, chosen_arm)
    # else :
    #   self.action += chosen_arm.T@chosen_arm
    #   self.invcov = np.linalg.pinv(self.cov + self.action)
    
    self.invcov =  np.linalg.pinv(self.cov)
      
    # update theta estimator
    self.theta_hat = np.matmul(self.invcov, self.history_rx)
    
    # update histories
    self.t += 1

  def name(self):
    return 'LinUCB'
  
  def __repr__(self):
    str = f'Object LinUCB'
    str += f'\nTime step = {self.t}'
    str += f'\nTheta Hat = {self.theta_hat}, shape = {self.theta_hat.shape}'
    str += f'\nCov matrix shape = {self.cov.shape}'
    
    return str








# policies

linucb = LinUCB(d, delta, sigma=sigma, lambda_reg=1.)
uniform = LinUniform()
e_greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0.1)
greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0.)


theta = ActionsGenerator(1,d).reshape(-1) # another way of getting a 'random vector' (depends on your implementation)
theta /= np.linalg.norm(theta)

iid_env = LinearBandit(theta, K, var=sigma**2)


iid_env


#Â linucb


# e_greedy


linucb_vs_greedy = experiment(iid_env, [linucb, greedy, e_greedy], Nmc=N, T=T,pseudo_regret=True)
plot_regret(linucb_vs_greedy)





# class LinTS ... (use you LinUCB template)












def play_fixed(environment, agent, Nmc, T, actions=None, pseudo_regret=True):
    """
    Play one Nmc trajectories over a horizon T for the specified agent.
    Return the agent's name (sring) and the collected data in an nd-array.
    actions: a fixed action set. Default is set to be the canonical basis.
    """

    data = np.zeros((Nmc, T))

    for n in range(Nmc):
        agent.reset()
        for t in range(T):
            # action_set = environment.get_action_set() -> We no longer call on your ActionsGenerator function
            action_set = np.copy(actions) # the actions given as input
            action = agent.get_action(action_set)
            reward = environment.get_reward(action)
            agent.receive_reward(action,reward)

            # compute instant (pseudo) regret
            means = environment.get_means()
            best_reward = np.max(means)
            if pseudo_regret:
              data[n,t] = best_reward - np.dot(environment.theta,action)
            else:
              data[n,t]= best_reward - reward # this can be negative due to the noise, but on average it's positive

    return agent.name(), data


def experiment_fixed(environment, agents, Nmc, T, actions=None, pseudo_regret=True):
    """
    Play Nmc trajectories for all agents over a horizon T. Store all the data in a dictionary.
    """

    all_data = {}
    if actions is None:
      actions = np.ActionsGenerator(K,d) #call it once!
    print(actions)

    for agent in agents:
        agent_id, regrets = play_fixed(environment, agent, Nmc, T, actions=actions, pseudo_regret=pseudo_regret)

        all_data[agent_id] = regrets

    return all_data





class UCB:
  def __init__(self, K, var):
      self.K = K
      self.var = var
      self.count_actions = np.zeros(self.K)
      self.count_rewards = np.zeros(self.K)
      self.t = 0

  def get_action(self,action_set):
      if self.t < self.K:
        action = self.t
      else:
        empirical_means = self.count_rewards / self.count_actions
        ucbs = np.sqrt(6 * self.var * np.log(self.t) / self.count_actions) # 6 could be replaced by a 2, try it out :)
        action = np.argmax(empirical_means + ucbs)

      self.t += 1
      self.count_actions[action] += 1
      self.current_action = action #need to remember the *index* of the action now
      return action_set[action,:]

  def receive_reward(self, action, reward):
      self.count_rewards[self.current_action] += reward

  def reset(self):
      self.count_actions = np.zeros(self.K)
      self.count_rewards = np.zeros(self.K)
      self.t = 0

  def name(self):
      return 'UCB('+str(self.var)+')'


d=3
K=7 # same as before, more actions than dimension

env = LinearBandit(theta, K)
ucb = UCB(K,var=1.)
linucb = LinUCB(d, delta=0.01, lambda_reg=1., sigma=1. )
reg_fixed_actions = experiment_fixed(env, [ucb, linucb], Nmc=10, T=200, actions=None, pseudo_regret=True)


plot_regret(reg_fixed_actions)





d=7
K=d
# theta = np.random.multivariate_normal(np.zeros(d),np.eye(d))
theta = np.linspace(0.1,1,num=d) # just d actions in increasing value order
#theta = your choice of parameter
theta /= np.linalg.norm(theta) #optional if you set theta with bounded norm :)

env = LinearBandit(theta, d, fixed_actions=np.eye(d))
ucb = UCB(d,var=1.)
linucb = LinUCB(d, delta=0.01, lambda_reg=1., sigma=1. )
reg_fixed_actions = experiment_fixed(env, [ucb, linucb], Nmc=10, T=200, actions=np.eye(d), pseudo_regret=True)


plot_regret(reg_fixed_actions)



