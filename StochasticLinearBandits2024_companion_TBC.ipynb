{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njfnT-sUER9J"
   },
   "source": [
    "# Lab on Stochastic Linear Bandits :\n",
    "\n",
    "We provide the environment to run a standard linear bandit experiment. The objective of this lab session is to understand how to implement LinUCB, the algorithm seen in class and its variant LinTS. We shall see that in practice there are some shortcomings in the implementation to make it efficient so we will guide you to obtain a working version.\n",
    "\n",
    "Questions are inline in the notebook and some reserved space are allocated for answers, but feel free to add cells for remarks and run your own experiments to test hypotheses you may have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ylZIq1mOJBr8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import bernoulli\n",
    "from math import log\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette('colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X6gL_zi6LaZh"
   },
   "outputs": [],
   "source": [
    "def plot_regret(regrets, logscale=False, lb=None,q=10):\n",
    "    \"\"\"\n",
    "    regrets must be a dict {'agent_id':regret_table}\n",
    "    \"\"\"\n",
    "\n",
    "    reg_plot = plt.figure()\n",
    "    #compute useful stats\n",
    "#     regret_stats = {}\n",
    "    for i, agent_id in enumerate(regrets.keys()):\n",
    "        data = regrets[agent_id]\n",
    "        N, T = data.shape\n",
    "        cumdata = np.cumsum(data, axis=1) # cumulative regret\n",
    "\n",
    "        mean_reg = np.mean(cumdata, axis=0)\n",
    "        q_reg = np.percentile(cumdata, q, axis=0)\n",
    "        Q_reg = np.percentile(cumdata, 100-q, axis=0)\n",
    "\n",
    "#         regret_stats[agent_id] = np.array(mean_reg, q_reg, Q_reg)\n",
    "\n",
    "        plt.plot(np.arange(T), mean_reg, color=colors[i], label=agent_id)\n",
    "        plt.fill_between(np.arange(T), q_reg, Q_reg, color=colors[i], alpha=0.2)\n",
    "\n",
    "    if logscale:\n",
    "        plt.xscale('log')\n",
    "        plt.xlim(left=100)\n",
    "\n",
    "    if lb is not None:\n",
    "        plt.plot(np.arange(T), lb, color='black', marker='*', markevery=int(T/10))\n",
    "\n",
    "    plt.xlabel('time steps')\n",
    "    plt.ylabel('Cumulative Regret')\n",
    "    plt.legend()\n",
    "    reg_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3phj8yu79_Rt"
   },
   "source": [
    "# Environment Class\n",
    "\n",
    "The environment class allows to create 3 types of linear bandit problems:\n",
    "* 'fixed': normally requires a fixed_actions input (otherwise randomly generated at start) which is kept all along the game;\n",
    "* 'iid': at each round, the environment samples K actions at random on the sphere.\n",
    "\n",
    "For each of these types of game, the class is used to generate the action sets at each round and the reward for a chosen action (chosen by an Agent, see the \"Play!\" section for the details of the interaction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH892IKv95t3"
   },
   "source": [
    "### Action generators\n",
    "Please implement a function that generates K actions in dimension d. You may want to check the lecture slides to see whether some conditions should be respected.\n",
    "\n",
    "In the report, explain and justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dKTTMwwB8rQS"
   },
   "outputs": [],
   "source": [
    "def ActionsGenerator(K,d, mean=None):\n",
    "    \"\"\"\n",
    "    K: int -- number of action vectors to be generated\n",
    "    d : int -- dimension of the action space\n",
    "    returns : an array of K vectors uniformly sampled on the unit sphere in R^d\n",
    "    \"\"\"\n",
    "    # Random vector with mean = 0\n",
    "    rand_vector = np.random.normal(size=(K, d))\n",
    "    \n",
    "    # Normalize each vector to have unit norm\n",
    "    norms = np.linalg.norm(rand_vector, axis=1, keepdims=True)\n",
    "    unit_vectors = rand_vector / norms\n",
    "\n",
    "    # If a mean vector is provided, add it to the unit_vector\n",
    "    if mean is not None:\n",
    "        unit_vectors += mean\n",
    "        \n",
    "        # Normalize again to ensure they remain on the unit sphere\n",
    "        norms = np.linalg.norm(unit_vectors, axis=1, keepdims=True)\n",
    "        unit_vectors = unit_vectors / norms\n",
    "\n",
    "    return unit_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkWi1Qaiztia"
   },
   "source": [
    "### Linear Bandit environment\n",
    "\n",
    "The following class is your environment: it generates an action set of K vectors at each round and returns the (random) reward given an action.\n",
    "You can see how it is used in the experiment function further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "zKlD-rMUKumJ"
   },
   "outputs": [],
   "source": [
    "class LinearBandit:\n",
    "\n",
    "    def __init__(self, theta, K, var=1., fixed_actions=None):\n",
    "      \"\"\"\n",
    "      theta: d-dimensional vector (bounded) representing the hidden parameter\n",
    "      K: number of actions per round (random action vectors generated each time)\n",
    "      pb_type: string in 'fixed', 'iid', 'nsr' (please ignore NotSoRandom)\n",
    "      \"\"\"\n",
    "      self.d = np.size(theta)\n",
    "      self.theta = theta\n",
    "      self.K = K\n",
    "      self.var = var\n",
    "      self.current_action_set = np.zeros(self.d)\n",
    "      self.fixed_actions = fixed_actions\n",
    "\n",
    "    def get_action_set(self):\n",
    "      \"\"\"\n",
    "      Generates a set of vectors in dimension self.d. Use your ActionsGenerator\n",
    "      Alternatively, the set of actions is fixed a priori (given as input).\n",
    "      Implement a condition to return the fixed set when one is given\n",
    "      \"\"\"\n",
    "      if self.fixed_actions is not None:\n",
    "        return self.fixed_actions\n",
    "      else: \n",
    "        self.current_action_set = ActionsGenerator(self.K, self.d)\n",
    "        return ActionsGenerator(self.K, self.d)\n",
    "\n",
    "\n",
    "    def get_reward(self, action):\n",
    "      \"\"\" sample reward given action and the model of this bandit environment\n",
    "      action: d-dimensional vector (action chosen by the learner)\n",
    "      \"\"\"\n",
    "      # self.current_action_set = action\n",
    "      mean = np.dot(action, self.theta)\n",
    "      return np.random.normal(mean, scale=self.var)\n",
    "\n",
    "    def get_means(self):\n",
    "      # print(f'current action_set shape = {self.current_action_set.shape}')\n",
    "      # print(f'theta shape = {self.theta.shape}')\n",
    "      return np.dot(self.current_action_set, self.theta)\n",
    "    \n",
    "    def __repr__(self):\n",
    "      str = 'Object Environnement'\n",
    "      str += f'\\n d = {self.d}'\n",
    "      str += f'\\n Theta = {self.theta}, shape = {self.theta.shape}'\n",
    "      str += f'\\n Current_action_set = {self.current_action_set}, shape = {self.current_action_set.shape}'\n",
    "      \n",
    "      return str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73lNhpBGwxFx"
   },
   "source": [
    "# Play !\n",
    "The function play runs one path of regret for one agent. The function experiment runs all agents several (Nmc) times and returns all the logged data. Feel free to check the inputs and outputs required when you decide on the implementation of your own agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "ChN6oiILAk2S"
   },
   "outputs": [],
   "source": [
    "def play(environment, agent, Nmc, T, pseudo_regret=True):\n",
    "    \"\"\"\n",
    "    Play one Nmc trajectories over a horizon T for the specified agent.\n",
    "    Return the agent's name (sring) and the collected data in an nd-array.\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.zeros((Nmc, T))\n",
    "\n",
    "    for n in range(Nmc):\n",
    "        agent.reset()\n",
    "        for t in range(T):\n",
    "            action_set = environment.get_action_set()\n",
    "            # print(f'action set shape = {action_set.shape}')\n",
    "            # print(f'env = {env}')\n",
    "            action = agent.get_action(action_set)\n",
    "            # print(f'action from get action = {action.shape}')\n",
    "            reward = environment.get_reward(action)\n",
    "            agent.receive_reward(action,reward)\n",
    "\n",
    "            # compute instant (pseudo) regret\n",
    "            means = environment.get_means()\n",
    "            best_reward = np.max(means)\n",
    "            if pseudo_regret:\n",
    "              # pseudo-regret removes some of the noise and corresponds to the metric studied in class\n",
    "              data[n,t] = best_reward - np.dot(environment.theta,action)\n",
    "            else:\n",
    "              data[n,t]= best_reward - reward # this can be negative due to the noise, but on average it's positive\n",
    "\n",
    "    return agent.name(), data\n",
    "\n",
    "\n",
    "def experiment(environment, agents, Nmc, T,pseudo_regret=True):\n",
    "    \"\"\"\n",
    "    Play Nmc trajectories for all agents over a horizon T. Store all the data in a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    all_data = {}\n",
    "\n",
    "    for agent in agents:\n",
    "        agent_id, regrets = play(environment, agent,Nmc, T,pseudo_regret)\n",
    "\n",
    "        all_data[agent_id] = regrets\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_PSPRfU-CRy"
   },
   "source": [
    "# Linear Bandit Agents\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ny1zhj6NwxFy"
   },
   "source": [
    "\n",
    "## LinUCB : Implementing optimism in $R^d$\n",
    "\n",
    "As seen in class, the actions are now vectors in $R^d$, representing contextual features, and the environment is assumed to generate rewards according to some hidden linear function $f_\\theta(a) = a^\\top \\theta$. The goal of the learner is thus to estimate $\\theta$ while keeping a measure of the uncertainty in all the directions of the feature space.\n",
    "\n",
    "* **Baseline: Implementation of LinEpsilonGreedy** In the next cell, we implemented a LinUniform Agent that returns one of the action vectors of the action set, chosen uniformly at random. Please implement a `LinEpsilonGreedy` agent and test it against `Greedy` ($\\epsilon=0$) on the 2 proposed  environments (iid and fixed actions). What do you notice? Is $\\epsilon$-Greedy a good baseline to test algorithms?\n",
    "\n",
    "\n",
    "* **Implementation of LinUCB**: you need to compute UCBs for each arm of the current action set received from the environment, but this time the exploration bonus depends on the history of taken actions and received rewards (see course material).\n",
    "\n",
    "* **Efficiency of the matrix inversion step**: One key step is to invert the covariance matrix in order to compute the elliptical norm of each available action. Remark however that at round $t+1$, the new covariance matrix is very similar to the previous one at rount $t$... Can you think of a way to optimize this step by simply updating the old one ? Please implement this improvement as an additional option to your `LinUCB` agent so you can compare runtimes from bandit problems in dimension $d=2,8,16,32,64$. Plot the result of the compared runtimes. In your report,  discuss the computational complexity of this step and the resulting improvement, give your new update formula, and report the plot of compared runtimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2qQCH4_wxFy"
   },
   "source": [
    "### Uniform random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "e9uEOaPwBAbA"
   },
   "outputs": [],
   "source": [
    "class LinUniform():\n",
    "  def __init__(self):\n",
    "    # self.d = d\n",
    "    # self.lambda_reg = lambda_reg\n",
    "    # self.reset()\n",
    "    pass\n",
    "  \n",
    "  def get_action(self, arms):\n",
    "    K, _ = arms.shape\n",
    "    return arms[np.random.choice(K)]\n",
    "\n",
    "  def receive_reward(self, chosen_arm, reward):\n",
    "    # chosen_arm = chosen_arm.reshape(-1, 1)\n",
    "\n",
    "    # self.action += chosen_arm.T@chosen_arm\n",
    "    # self.invcov = np.linalg.pinv(self.cov + self.action) \n",
    "\n",
    "\n",
    "    # #update b_t\n",
    "    # self.b_t += reward * chosen_arm\n",
    "\n",
    "    # self.hat_theta = self.invcov@ self.b_t # update the least square estimate\n",
    "    pass\n",
    "  \n",
    "  def reset(self):\n",
    "    # self.hat_theta = np.zeros(self.d).reshape(-1, 1)\n",
    "    # self.cov = self.lambda_reg * np.identity(self.d)\n",
    "    # self.invcov = np.identity(self.d)\n",
    "    # self.b_t = np.zeros(self.d).reshape(-1, 1)\n",
    "    # self.action = np.zeros(self.d).reshape(-1, 1)\n",
    "    pass\n",
    "  def name(self):\n",
    "    return 'Unif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SCdWRH0wxFy"
   },
   "source": [
    "### Lin-$\\epsilon$-Greedy policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "XC8Zp87qwxFy"
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy.linalg import pinv\n",
    "from time import time\n",
    "\n",
    "class LinEpsilonGreedy:\n",
    "  def __init__(self, d,lambda_reg, eps=0.1, other_option=None):\n",
    "    self.eps = eps # exploration probability\n",
    "    self.d = d\n",
    "    self.lambda_reg = lambda_reg\n",
    "    self.reset()\n",
    "    self.other_option = other_option\n",
    "    #use other inputs if needed\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    This function should reset all estimators and counts.\n",
    "    It is used between independent experiments (see 'Play!' above)\n",
    "    \"\"\"\n",
    "    self.t = 0\n",
    "    self.hat_theta = np.zeros(self.d).reshape(-1, 1)\n",
    "    self.cov = self.lambda_reg * np.identity(self.d)\n",
    "    self.invcov = np.identity(self.d)\n",
    "    self.b_t = np.zeros(self.d).reshape(-1, 1)\n",
    "    self.action = np.zeros(self.d).reshape(-1, 1)\n",
    "    \n",
    "\n",
    "  def greedy_or_not(self, K, arms):\n",
    "    \"\"\"This function picks the greedy arm with a probability (p = 1 - epsilon)\n",
    "    It tries a random arm with a probability (p = epsilon)\"\"\"\n",
    "\n",
    "    threshold = np.random.rand() # Pick random numbers between 0 and 1\n",
    "\n",
    "    if self.eps < threshold: # Greedy strategy, pick the arm that provides the best rewards\n",
    "      expected_reward = arms@self.hat_theta\n",
    "      index = np.argmax(expected_reward) # changer ça\n",
    "      return arms[index, :]\n",
    "    else : # Explore strategy, pick a random arm\n",
    "      K, _ = arms.shape\n",
    "      return arms[np.random.choice(K)]\n",
    "\n",
    "\n",
    "  def get_action(self, arms):\n",
    "    K = arms.shape[0] # number of arms\n",
    "\n",
    "    action = self.greedy_or_not(K, arms)\n",
    "    return action\n",
    "  \n",
    "  def sherman_morrison(self, P_inv, a):\n",
    "      denominator = (1 + a.T@P_inv@a)\n",
    "      numerator = (P_inv@a)@(a.T@P_inv)\n",
    "      return numerator/denominator\n",
    "  \n",
    "  def receive_reward(self, chosen_arm, reward):\n",
    "    \"\"\"\n",
    "    update the internal quantities required to estimate the parameter theta using least squares\n",
    "    \"\"\"\n",
    "    #update inverse covariance matrix\n",
    "    chosen_arm = chosen_arm.reshape(-1, 1)\n",
    "    if self.other_option is not None :\n",
    "      if self.t == 0:\n",
    "        self.invcov = np.linalg.pinv(self.cov) \n",
    "        # self.invcov -= np.copy(self.sherman_morrison(self.invcov, chosen_arm))\n",
    "      else :\n",
    "        self.invcov -= self.sherman_morrison(self.invcov, chosen_arm)\n",
    "    else :\n",
    "      self.action += chosen_arm.T@chosen_arm\n",
    "      self.invcov = np.linalg.pinv(self.cov + self.action) \n",
    "\n",
    "    #update b_t\n",
    "    self.b_t += reward * chosen_arm\n",
    "\n",
    "    self.hat_theta = self.invcov@self.b_t # update the least square estimate\n",
    "    self.t += 1\n",
    "\n",
    "  def name(self):\n",
    "    return 'LinEGreedy('+str(self.eps)+')'\n",
    "  \n",
    "  def __repr__(self):\n",
    "    str = f'Object LinEpsilonGreedy'\n",
    "    str += f'\\nTime step = {self.t}'\n",
    "    str += f'\\nTheta Hat = {self.hat_theta}, shape = {self.hat_theta.shape}'\n",
    "    str += f'\\nCov matrix = {self.cov}, shape = {self.cov.shape}'\n",
    "    \n",
    "    return str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRY4RXMo1nUX"
   },
   "source": [
    "### Default setting\n",
    "\n",
    "Quick test: I propose some simple settings to run a quick test.\n",
    "For your report, feel free to change these and explain / justify your choices.\n",
    "Please report the resulting plot and your conclusions on Lin-E-Greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "UUKH70ENPUMO"
   },
   "outputs": [],
   "source": [
    "d = 15  # dimension\n",
    "K = 30  # number of arms\n",
    "\n",
    "# parametor vector \\theta, normalized :\n",
    "theta = ActionsGenerator(1,d).reshape(-1) # another way of getting a 'random vector' (depends on your implementation)\n",
    "# theta = np.array([0.45, 0.5, 0.5])\n",
    "theta /= np.linalg.norm(theta)\n",
    "T = 3  # Finite Horizon\n",
    "N = 2  # Monte Carlo simulations\n",
    "\n",
    "delta = 0.1 # could be set directly in the algorithms\n",
    "sigma = 1.\n",
    "\n",
    "#choice of percentile display\n",
    "q = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "Rye2g9lgQHD6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Environnement\n",
      " d = 15\n",
      " Theta = [ 0.17104176 -0.12780983  0.05430021 -0.37973898 -0.23032588  0.22736794\n",
      " -0.55385881 -0.25058229  0.08835728  0.20628349  0.06120807  0.01363562\n",
      " -0.11288705 -0.43321885 -0.27972771], shape = (15,)\n",
      " Current_action_set = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape = (15,)\n"
     ]
    }
   ],
   "source": [
    "env = LinearBandit(theta, K, var=sigma**2)\n",
    "\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "-Fa2aJV7SIze"
   },
   "outputs": [],
   "source": [
    "# policies\n",
    "\n",
    "uniform = LinUniform()\n",
    "e_greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0.1, other_option=None)\n",
    "greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0., other_option=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "MCaCjjshSNse"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n",
      "action from get action = (15,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8825/3942893175.py:33: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  reg_plot.show()\n"
     ]
    }
   ],
   "source": [
    "exp = experiment(env, [greedy, e_greedy, uniform], Nmc=N, T=T,pseudo_regret=True)\n",
    "plot_regret(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAVBQ2k5wxF0"
   },
   "source": [
    "Is Lin-E-Greedy a strong baseline? Is it hard to beat?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epsilon greedy algorithm was tested on a simple problem where $\\theta$ is defined as a random vector between 0 and 1 and all the actions are generated randomly on a unit sphere. This algorithm is naive as it keeps on exploring when all states are visited, which causes the regret to grow when $\\epsilon > 0$. To increase performance, we would expect to use an algorithm that explores less over time. Therefore, despite its simplicity, the epsilon greedy algorithm is a good baseline to compare with more sophisticated algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix inversion complexity discussion\n",
    "\n",
    "The matrix inversion complexity is $O(d^3)$. Therefore, as d increases, this matrix inversion becomes the bottleneck of the epsilon greedy algorithm. To improve performance, we could use the sherman morrison method that has $O(d^2)$ complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8825/3942893175.py:33: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  reg_plot.show()\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "d_vector = np.array([3, 10, 50, 100])\n",
    "time_vector = np.zeros((2, len(d_vector)))\n",
    "\n",
    "# Computational time comparison of both matrix inversion techniques\n",
    "for i, d_val in enumerate(d_vector):\n",
    "    theta = ActionsGenerator(1,d_val).reshape(-1)\n",
    "    env_time = LinearBandit(theta, K, var=sigma**2)\n",
    "    \n",
    "    start = time.time()\n",
    "    greedy_1 = LinEpsilonGreedy(d_val, lambda_reg=1., eps=0.1, other_option = True)\n",
    "    exp1 = experiment(env_time, [greedy_1], Nmc=N, T=T,pseudo_regret=True)\n",
    "    end = time.time() - start\n",
    "    plot_regret(exp1)\n",
    "    time_vector[0, i] = end\n",
    "\n",
    "    start_2 = time.time()\n",
    "    greedy_2 = LinEpsilonGreedy(d_val, lambda_reg=1., eps=0.1)\n",
    "    exp2 = experiment(env_time, [greedy_2], Nmc=N, T=T,pseudo_regret=True)\n",
    "    end_2 = time.time() - start_2\n",
    "    time_vector[1, i] = end_2\n",
    "    plot_regret(exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Epsilon greedy computational time comparison')"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(d_vector, time_vector[0,:], label = \"Sherman Morrison formula\")\n",
    "plt.semilogy(d_vector, time_vector[1,:], label = \"Classical matrix inversion\")\n",
    "plt.xlabel(\"d dimension\")\n",
    "plt.ylabel(\"Computational time - s\")\n",
    "plt.legend()\n",
    "plt.title(\"Epsilon greedy computational time comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUyi4C_OwxFy"
   },
   "source": [
    "## Lin-UCB: The optimistic way\n",
    "\n",
    "**Implement LinUCB** as seen in class and test it against the baselines implemented above.\n",
    "\n",
    "If you are happy with the result, move to the last part below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "sVfDJvY4LCSE"
   },
   "outputs": [],
   "source": [
    "class LinUCB:\n",
    "    \n",
    "  # constructeur - init paramètres :\n",
    "  # - d = dimension\n",
    "  # - lambda_reg = regularization parameter\n",
    "  # - ??\n",
    "  def __init__(self, d, delta, sigma=sigma, lambda_reg=1., other_option=None, T=1000):\n",
    "      \n",
    "    self.d = d\n",
    "    self.delta = delta\n",
    "    self.sigma = sigma\n",
    "    self.lambda_reg = lambda_reg\n",
    "    self.T = T\n",
    "    \n",
    "    self.other_option = other_option   \n",
    "    self.reset()\n",
    "\n",
    "  # reset method - used in the play function loop\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    This function should reset all estimators and counts.\n",
    "    It is used between independent experiments (see 'Play!' above)\n",
    "    \"\"\"\n",
    "    self.t = 0\n",
    "    self.theta_hat = np.zeros(self.d).reshape(-1, 1)\n",
    "    self.cov = self.lambda_reg * np.identity(self.d)\n",
    "    self.invcov = np.identity(self.d)\n",
    "    self.history_design_matrix = np.zeros((d,d)) # sum over s of x_s . x_s^{T}\n",
    "    self.history_rx = np.zeros(d) # sum over s of r_s . x_s\n",
    "\n",
    "  def _sherman_morrison(self, P_inv, a):\n",
    "      denominator = (1 + a.T@P_inv@a)\n",
    "      numerator = (P_inv@a)@(a.T@P_inv)\n",
    "      return numerator/denominator\n",
    "    \n",
    "  def _compute_ucbs(self, K, arms):\n",
    "          \n",
    "      # calculate UCB bounds\n",
    "      ucbs = np.zeros(K)\n",
    "      for k in range(K):\n",
    "          action = arms[k]\n",
    "          beta = self.sigma * np.sqrt(2*np.log(self.T) + self.d * np.log(1+self.t / (self.d * self.lambda_reg)) /\n",
    "                                 + np.sqrt(self.lambda_reg))\n",
    "          norm_action = np.sqrt(np.matmul(action.T, np.matmul(self.invcov, action)))\n",
    "          # print(f'action shape = {action.shape}')\n",
    "          # print(f'self.theta_hat reshape = {self.theta_hat.reshape(-1,).shape}')\n",
    "          ucbs[k] = np.dot(action, self.theta_hat.reshape(-1,)) + norm_action * beta\n",
    "          \n",
    "      chosen_arm_id = np.argmax(ucbs)\n",
    "      chosen_action = arms[chosen_arm_id].reshape(-1,)\n",
    "      \n",
    "      return chosen_arm_id, chosen_action\n",
    "\n",
    "  # choose the arm per policy\n",
    "  def get_action(self, arms):\n",
    "      \n",
    "    K = arms.shape[0] # number of arms\n",
    "    chosen_arm_id, chosen_action = self._compute_ucbs(K, arms)\n",
    "    # print(chosen_action)\n",
    "    return chosen_action\n",
    "  \n",
    "  # update internal parameters\n",
    "  def receive_reward(self, chosen_arm, reward):\n",
    "    \"\"\"\n",
    "    update the internal quantities required to estimate the parameter theta using least squares\n",
    "    \"\"\"\n",
    "    \n",
    "    # update sum of xx.T\n",
    "    self.history_design_matrix += np.matmul(chosen_arm.T, chosen_arm)\n",
    "    # update sum of rx\n",
    "    # print(f'reward = {reward}')\n",
    "    # print(f'history_rx = {self.history_rx}.shape')\n",
    "    self.history_rx += reward * chosen_arm\n",
    "    # update cov matrix\n",
    "    self.cov = self.lambda_reg * np.identity(self.d) + self.history_design_matrix # B_t_lambda\n",
    "    \n",
    "    #update inverse covariance matrix, possibly with sherman-morrisson\n",
    "    # chosen_arm = chosen_arm.reshape(-1, 1)\n",
    "    \n",
    "    # if self.other_option is not None :\n",
    "    #   if self.t == 0:\n",
    "    #     self.invcov = np.linalg.pinv(self.cov) \n",
    "    #   else :\n",
    "    #     self.invcov -= self._sherman_morrison(self.invcov, chosen_arm)\n",
    "    # else :\n",
    "    #   self.action += chosen_arm.T@chosen_arm\n",
    "    #   self.invcov = np.linalg.pinv(self.cov + self.action)\n",
    "    self.invcov =  np.linalg.pinv(self.cov)\n",
    "      \n",
    "    # update theta estimator\n",
    "    self.theta_hat = np.matmul(self.invcov, self.history_rx)\n",
    "    \n",
    "    # update histories\n",
    "    self.t += 1\n",
    "\n",
    "  def name(self):\n",
    "    return 'LinUCB'\n",
    "  \n",
    "  def __repr__(self):\n",
    "    str = f'Object LinUCB'\n",
    "    str += f'\\nTime step = {self.t}'\n",
    "    str += f'\\nTheta Hat = {self.theta_hat}, shape = {self.theta_hat.shape}'\n",
    "    str += f'\\nCov matrix shape = {self.cov.shape}'\n",
    "    \n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPSEMvWhwxF1"
   },
   "source": [
    "### Test against baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "hqGTqj2-wxFz"
   },
   "outputs": [],
   "source": [
    "# policies\n",
    "\n",
    "linucb = LinUCB(d, delta, sigma=sigma, lambda_reg=1.)\n",
    "uniform = LinUniform()\n",
    "e_greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0.1)\n",
    "greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = ActionsGenerator(1,d).reshape(-1) # another way of getting a 'random vector' (depends on your implementation)\n",
    "theta /= np.linalg.norm(theta)\n",
    "\n",
    "iid_env = LinearBandit(theta, K, var=sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Object Environnement\n",
       " d = 15\n",
       " Theta = [-0.15487514 -0.12260666 -0.1005312  -0.00389548 -0.15721351 -0.0817665\n",
       " -0.55792103 -0.2544118  -0.0751942   0.22259529  0.08729594  0.13919493\n",
       " -0.10002854  0.66243962  0.11147161], shape = (15,)\n",
       " Current_action_set = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape = (15,)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iid_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Object LinUCB\n",
       "Time step = 0\n",
       "Theta Hat = [[0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]], shape = (15, 1)\n",
       "Cov matrix shape = (15, 15)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jej__TQwwxF1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8825/3942893175.py:33: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  reg_plot.show()\n"
     ]
    }
   ],
   "source": [
    "linucb_vs_greedy = experiment(iid_env, [linucb, greedy, e_greedy], Nmc=N, T=T,pseudo_regret=True) # greedy, e_greedy, \n",
    "plot_regret(linucb_vs_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8p_flT3gwxFz"
   },
   "source": [
    "## LinTS : Taking the Bayesian way\n",
    "\n",
    "Thompson Sampling is a popular bayesian alternative to the standard optimistic bandit algorithms (see Chapter 36 of Bandit Algorithms). The key idea is to rely on Bayesian *samples* to get a proxy for the hidden parameter $\\theta$ of the problem instead of building high-probability confidence regions.\n",
    "\n",
    "* **Posterior derivation**: Let us place a Gaussian prior with mean $\\mathbf{0}$ and covariance $\\lambda I$ on $\\theta$. Given actions $A_1,\\ldots,A_t$ and rewards $Y_1,\\ldots,Y_t$, Can you compute the expression of the posterior at the beginning of round $t+1$ ?\n",
    "\n",
    "In your report, write the distribution of the posterior as a function of the prior and the observed data. No need to report your full derivation if you are lacking space.\n",
    "\n",
    "\n",
    "* **Implementation of LinTS**. Please implement Linear Thompson Sampling using the formula you derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_navr3GuwxFz"
   },
   "outputs": [],
   "source": [
    "# class LinTS ... (use you LinUCB template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_t2LC5HFwxFz"
   },
   "source": [
    "### Comparison and report\n",
    "\n",
    "Compare LinUCB, LinTS and LinEGreedy on a problem of your choice. In your report, explain your choice of problem and report the plot of your experiment as well as a few sentences of comment: Is there a clear 'winner'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIXTRA9UNn5H"
   },
   "source": [
    "#Bonus\n",
    "\n",
    "In this bonus part, we explore the role of the action sets on the performance of our algorithms.\n",
    "\n",
    "In class we said that action sets can be 'arbitrary'. This means that, in principle, they do not have to follow a distribution, they do not have to be random either.\n",
    "\n",
    "What happens if the action set is fixed?\n",
    "\n",
    "We propose an alternative 'play' function that fixes the action set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gAnyXJnOHJq"
   },
   "outputs": [],
   "source": [
    "def play_fixed(environment, agent, Nmc, T, actions=None, pseudo_regret=True):\n",
    "    \"\"\"\n",
    "    Play one Nmc trajectories over a horizon T for the specified agent.\n",
    "    Return the agent's name (sring) and the collected data in an nd-array.\n",
    "    actions: a fixed action set. Default is set to be the canonical basis.\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.zeros((Nmc, T))\n",
    "\n",
    "    for n in range(Nmc):\n",
    "        agent.reset()\n",
    "        for t in range(T):\n",
    "            # action_set = environment.get_action_set() -> We no longer call on your ActionsGenerator function\n",
    "            action_set = np.copy(actions) # the actions given as input\n",
    "            action = agent.get_action(action_set)\n",
    "            reward = environment.get_reward(action)\n",
    "            agent.receive_reward(action,reward)\n",
    "\n",
    "            # compute instant (pseudo) regret\n",
    "            means = environment.get_means()\n",
    "            best_reward = np.max(means)\n",
    "            if pseudo_regret:\n",
    "              data[n,t] = best_reward - np.dot(environment.theta,action)\n",
    "            else:\n",
    "              data[n,t]= best_reward - reward # this can be negative due to the noise, but on average it's positive\n",
    "\n",
    "    return agent.name(), data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rz1x0DhNO97W"
   },
   "outputs": [],
   "source": [
    "def experiment_fixed(environment, agents, Nmc, T, actions=None, pseudo_regret=True):\n",
    "    \"\"\"\n",
    "    Play Nmc trajectories for all agents over a horizon T. Store all the data in a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    all_data = {}\n",
    "    if actions is None:\n",
    "      actions = np.ActionsGenerator(K,d) #call it once!\n",
    "    print(actions)\n",
    "\n",
    "    for agent in agents:\n",
    "        agent_id, regrets = play_fixed(environment, agent, Nmc, T, actions=actions, pseudo_regret=pseudo_regret)\n",
    "\n",
    "        all_data[agent_id] = regrets\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCQ8xr5NO-pL"
   },
   "source": [
    "Now the actions are fixed, so we could actually use UCB to address the problem: after all, it is just a K-armed bandit, but with structure.\n",
    "\n",
    "**When is LinUCB better than UCB?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtRBD1QFPrcN"
   },
   "outputs": [],
   "source": [
    "class UCB:\n",
    "  def __init__(self, K, var):\n",
    "      self.K = K\n",
    "      self.var = var\n",
    "      self.count_actions = np.zeros(self.K)\n",
    "      self.count_rewards = np.zeros(self.K)\n",
    "      self.t = 0\n",
    "\n",
    "  def get_action(self,action_set):\n",
    "      if self.t < self.K:\n",
    "        action = self.t\n",
    "      else:\n",
    "        empirical_means = self.count_rewards / self.count_actions\n",
    "        ucbs = np.sqrt(6 * self.var * np.log(self.t) / self.count_actions) # 6 could be replaced by a 2, try it out :)\n",
    "        action = np.argmax(empirical_means + ucbs)\n",
    "\n",
    "      self.t += 1\n",
    "      self.count_actions[action] += 1\n",
    "      self.current_action = action #need to remember the *index* of the action now\n",
    "      return action_set[action,:]\n",
    "\n",
    "  def receive_reward(self, action, reward):\n",
    "      self.count_rewards[self.current_action] += reward\n",
    "\n",
    "  def reset(self):\n",
    "      self.count_actions = np.zeros(self.K)\n",
    "      self.count_rewards = np.zeros(self.K)\n",
    "      self.t = 0\n",
    "\n",
    "  def name(self):\n",
    "      return 'UCB('+str(self.var)+')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1rf1eYyRzJS"
   },
   "outputs": [],
   "source": [
    "d=3\n",
    "K=7 # same as before, more actions than dimension\n",
    "\n",
    "env = LinearBandit(theta, K)\n",
    "ucb = UCB(K,var=1.)\n",
    "linucb = LinUCB(d, delta=0.01, lambda_reg=1., sigma=1. )\n",
    "reg_fixed_actions = experiment_fixed(env, [ucb, linucb], Nmc=10, T=200, actions=None, pseudo_regret=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4fjn4FrSXgJ"
   },
   "outputs": [],
   "source": [
    "plot_regret(reg_fixed_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QD3Si1xIWJG1"
   },
   "source": [
    "In general, LinUCB is better in such problem because it shares information across actions.\n",
    "\n",
    "Now, what if K=d and the fixed action set is exactly the canonical basis (K=d independent actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ME-Hn3ryVb8e"
   },
   "outputs": [],
   "source": [
    "d=7\n",
    "K=d\n",
    "# theta = np.random.multivariate_normal(np.zeros(d),np.eye(d))\n",
    "theta = np.linspace(0.1,1,num=d) # just d actions in increasing value order\n",
    "#theta = your choice of parameter\n",
    "theta /= np.linalg.norm(theta) #optional if you set theta with bounded norm :)\n",
    "\n",
    "env = LinearBandit(theta, d, fixed_actions=np.eye(d))\n",
    "ucb = UCB(d,var=1.)\n",
    "linucb = LinUCB(d, delta=0.01, lambda_reg=1., sigma=1. )\n",
    "reg_fixed_actions = experiment_fixed(env, [ucb, linucb], Nmc=10, T=200, actions=np.eye(d), pseudo_regret=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BokgjamhbhtV"
   },
   "outputs": [],
   "source": [
    "plot_regret(reg_fixed_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTZPar2nbjqZ"
   },
   "source": [
    "The two algorithms should be roughly on par. In fact, it is possible to refine UCB to get slightly better performance (by tightening a bit the upper confidence bounds).\n",
    "\n",
    "In \"[The End of Optimism](https://arxiv.org/pdf/1610.04491)\", Lattimore et al. show that for a certain type of action set, one can show that UCB has **linear regret**. A famous 'counter-example' (bad action set) is given in Section 8 therein.\n",
    "\n",
    "**Exercise**: find the action set (3 actions in dimension 2) and run the experiment above, what do you see? In (the appendix of) your report, please report your figures and conclusions.\n",
    "\n",
    "The problem of finding a good algorithm for the 'arbitrary' setting and the 'fixed-design' setting was open for a long time but recent papers (e.g. [Kirschner et al.,2021](https://arxiv.org/abs/2011.05944)) have now proposed solutions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rl_mva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
